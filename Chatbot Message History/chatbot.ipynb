{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c62bdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key= os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61593f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000025BC2DDE1E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000025BC51CE9C0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model= ChatGroq(model=\"openai/gpt-oss-120b\", api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebaf1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6cf8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Swajan**.  \\n\\nYou mentioned that you‚Äôre learning\\u202fGen\\u202fAI, so you‚Äôre probably:\\n\\n- Exploring the basics of generative models (like GPT, diffusion models, etc.)  \\n- Trying out some hands‚Äëon experiments with prompts, code, or toolkits such as Hugging\\u202fFace, LangChain, or OpenAI‚Äôs API  \\n- Reading up on topics like prompt engineering, model fine‚Äëtuning, safety, and evaluation  \\n\\nIf you‚Äôd like, I can help you:\\n\\n1. **Pick a next learning step** ‚Äì e.g., a quick project, a tutorial, or a deeper dive into a specific concept.  \\n2. **Find resources** ‚Äì articles, videos, notebooks, or courses tailored to your current level.  \\n3. **Answer questions** ‚Äì anything from ‚Äúwhat‚Äôs the difference between a decoder‚Äëonly and an encoder‚Äëdecoder model?‚Äù to ‚Äúhow do I fine‚Äëtune a small language model on my own data?‚Äù\\n\\nJust let me know what you‚Äôd like to focus on next!', additional_kwargs={'reasoning_content': 'The user asks: \"Hey what is my name? what do i do?\" The conversation: user introduced themselves as Swajan, learning Gen AI. So answer: name is Swajan. Also ask what they do? They are learning Gen AI. Maybe respond with a helpful suggestion. Also ask if they need help. Provide supportive tone.'}, response_metadata={'token_usage': {'completion_tokens': 292, 'prompt_tokens': 120, 'total_tokens': 412, 'completion_time': 0.698552625, 'completion_tokens_details': {'reasoning_tokens': 69}, 'prompt_time': 0.006625192, 'prompt_tokens_details': None, 'queue_time': 0.054175967, 'total_time': 0.705177817}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_70d048ba3c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b2af4-bcf0-7123-a4a5-029ee8a13dc4-0', usage_metadata={'input_tokens': 120, 'output_tokens': 292, 'total_tokens': 412, 'output_token_details': {'reasoning': 69}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi my name is Swajan I am learning Gen AI\"),\n",
    "        AIMessage(content=\"Hey Swajan! Nice meeting you. Really excited about your journey! Happy learning!\"),\n",
    "        HumanMessage(content=\"Hey what is my name? what do i do?\")    \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151dae7a",
   "metadata": {},
   "source": [
    "### Message History\n",
    "\n",
    "We can use Message History class to wrap our model and make it stateful. This wll kepp track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afedb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    \n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "        \n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history= RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba6d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a74e36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [\n",
    "       HumanMessage(content=\"Hi my name is Swajan I am learning Gen AI\"), \n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6de3622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Swajan! üëã It‚Äôs great to meet you, and welcome to the world of Generative AI. How can I help you on your learning journey? Are you interested in:\\n\\n- The fundamentals (models, training data, diffusion, transformers, etc.)  \\n- Practical hands‚Äëon projects (text generation, image synthesis, code assistants)  \\n- Specific tools and frameworks (TensorFlow, PyTorch, Hugging Face, LangChain, Stable Diffusion, etc.)  \\n- Ethics, prompt engineering, or deployment strategies  \\n\\nFeel free to let me know what you‚Äôre curious about or any particular question you have, and I‚Äôll do my best to guide you! üöÄ'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7df11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [\n",
    "       HumanMessage(content=\"What is my name\"), \n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb54dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is **Swajan**.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a75dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [\n",
    "       HumanMessage(content=\"what is my name?\"), \n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe9b677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don‚Äôt actually know your name‚Äîunless you‚Äôve shared it with me in this conversation, I don‚Äôt have that information. If you‚Äôd like me to address you by name, just let me know what you‚Äôd prefer!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd24afa",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Prompt Templates help to turn raw user information inot a format that the LLM can work with. In this case, the raw user input is just a message which we are passing to the LLM. Lets now make that a bit more complicated. First, lets add in a system message with some custom instrcutions. Next we'll add in more input besides the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d326ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the questions to the best of your capacity\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb84feb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Swajan! Nice to meet you. How can I assist you today?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"Hi my name is Swajan!\")],\n",
    "    \n",
    "})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1129b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the questions to the best of your capacity in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template|model\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history, input_messages_key=\"messages\")\n",
    "config={\n",
    "    \"configurable\":{\n",
    "        \"session_id\":\"chat5\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d705c8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã ‡¶∂‡ßç‡¶Ø‡¶æ‡¶Æ! ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶≤‡¶æ‡¶ó‡¶≤‡•§ ‡¶Ü‡¶ú ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response= with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":[HumanMessage(content=\"Hey my name is Shyam\")],\n",
    "        \"language\":\"Bangla\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756dbbd5",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "\n",
    "trim_messages is a helper to reduce how many messages we are sending to the model. The trimmer allows us to specify how many token we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd2301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi my name is Swajan', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi Swajan nice to meet you!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='nice to meet you too. can you tell me about AI', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure i can help you understand AI. Lets begin', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='That would be great. I like Vanilla Ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "\n",
    "trimmer= trim_messages(max_tokens=100,\n",
    "                       strategy=\"last\",\n",
    "                       token_counter=model,\n",
    "                       include_system= True,\n",
    "                       allow_partial=False,\n",
    "                       start_on=\"human\")\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi my name is Swajan\"),\n",
    "    AIMessage(content=\"Hi Swajan nice to meet you!\"),\n",
    "    HumanMessage(content=\"nice to meet you too. can you tell me about AI\"),\n",
    "    AIMessage(content=\"Sure i can help you understand AI. Lets begin\"),\n",
    "    HumanMessage(content=\"That would be great. I like Vanilla Ice cream\"),\n",
    "    AIMessage(content=\"yes!\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e35216fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapper={\n",
      "  messages: RunnableLambda(itemgetter('messages'))\n",
      "            | RunnableLambda(...)\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You mentioned that you like vanilla ice cream. üç¶'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RunnablePassthrough lets you surgically modify parts of the input \n",
    "# while guaranteeing the rest of the data keeps flowing untouched through the chain.\n",
    "\n",
    "# itemgetter(\"messages\"): Grabs the full list of messages (history + current input).\n",
    "\n",
    "# | trimmer: Immediately passes that list through the \"Bouncer\" to shorten it.\n",
    "\n",
    "# assign(messages=...): Takes the shortened list and overwrites the original messages key in your data bag.\n",
    "\n",
    "print(RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer))\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)|prompt_template|model\n",
    ")\n",
    "\n",
    "response=chain.invoke({\n",
    "    \"messages\": messages + [HumanMessage(content=\"what ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7abded9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Swajan.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config={\n",
    "    \"configurable\":\n",
    "        {\n",
    "            \"session_id\":\"chat6\"\n",
    "        }\n",
    "}\n",
    "\n",
    "response= with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is my name\")],\n",
    "        \"language\":\"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c79a95",
   "metadata": {},
   "source": [
    "## Vectors and Retrievers\n",
    "\n",
    "Documents\n",
    "Vector Stores\n",
    "Retrievers\n",
    "\n",
    "### Documents\n",
    "\n",
    "Langchain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has two attributes:\n",
    "\n",
    "page_content: a string representing the content;\n",
    "metadata: a dict containing arbitrary metadata;\n",
    "\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note than an individual Document object often represent a chunk of a larger document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b54fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents=[\n",
    "    Document(\n",
    "        page_content=\"I love samosa\",\n",
    "        metadata={\"source\":\"foods-india\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love cats\",\n",
    "        metadata={\"source\":\"pets-india\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love dogs\",\n",
    "        metadata={\"source\":\"pets-india\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love pav-bhaji\",\n",
    "        metadata={\"source\":\"foods-india\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love football\",\n",
    "        metadata={\"source\":\"sports-india\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e44374c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'foods-india'}, page_content='I love samosa'), Document(metadata={'source': 'foods-india'}, page_content='I love rosogolla'), Document(metadata={'source': 'foods-india'}, page_content='I love sambar'), Document(metadata={'source': 'foods-india'}, page_content='I love pav-bhaji'), Document(metadata={'source': 'foods-india'}, page_content='I love sev-puri')]\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f571108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in d:\\projects\\python\\genai\\langchain\\.venv\\lib\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade transformers sentence-transformers torch\n",
    "%pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51fac9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f38c5fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x25bc4839640>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector Store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store= Chroma.from_documents(documents=documents,embedding=embeddings)\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56dd3e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1338a3da-dc53-47fd-bae9-851177d948f1', metadata={'source': 'foods-india'}, page_content='I love pav-bhaji'),\n",
       " Document(id='bdbb98f9-5d3f-4516-8d83-6bd7ea180812', metadata={'source': 'foods-india'}, page_content='I love pav-bhaji'),\n",
       " Document(id='4aec3cf1-1ab6-4384-9af1-d7d1a00106d6', metadata={'source': 'foods-india'}, page_content='I love samosa'),\n",
       " Document(id='ae35c61e-89e3-4a25-bf38-c8eacbf66d08', metadata={'source': 'foods-india'}, page_content='I love samosa')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"north indian food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6691194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='4f013d5a-ba0e-4839-988e-e2f4f5cdc7dc', metadata={'source': 'sports-india'}, page_content='I love football'),\n",
       "  1.2660973072052002),\n",
       " (Document(id='1338a3da-dc53-47fd-bae9-851177d948f1', metadata={'source': 'foods-india'}, page_content='I love pav-bhaji'),\n",
       "  1.539424180984497),\n",
       " (Document(id='bdbb98f9-5d3f-4516-8d83-6bd7ea180812', metadata={'source': 'foods-india'}, page_content='I love pav-bhaji'),\n",
       "  1.539424180984497),\n",
       " (Document(id='deaa49f1-19ad-48f2-9f0a-b16710fe315a', metadata={'source': 'pets-india'}, page_content='I love dogs'),\n",
       "  1.6288678646087646)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_score(\"cricket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1152f",
   "metadata": {},
   "source": [
    "### Retrievers\n",
    "\n",
    "Langchain VectorStore objects do not subclass Runnable, so cannot immedately be integrated into LCEL chains.\n",
    "\n",
    "Langchain Retrievers are Runnables, so they implement a standard set of methods( synchronous and asynchronous invoke and batch ops) and are desugned to be incorporated in LCEL chains.\n",
    "\n",
    "We can create a simple version of this ourselves without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity search method.\n",
    "\n",
    "So a vector store into a retirever so that it can be a runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "246ecc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='4f013d5a-ba0e-4839-988e-e2f4f5cdc7dc', metadata={'source': 'sports-india'}, page_content='I love football')],\n",
       " [Document(id='a649b28f-f838-427a-9307-efb531dd8700', metadata={'source': 'pets-india'}, page_content='I love cats')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever= RunnableLambda(vector_store.similarity_search).bind(k=1)\n",
    "retriever.batch([\"football\",\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe2ba0",
   "metadata": {},
   "source": [
    "VectorStores implement an as_retriever method that will geerate a Retriever, specifically a VectorStoreRetriever. These retrievers include search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance we can replicate the above with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96ff7fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='a649b28f-f838-427a-9307-efb531dd8700', metadata={'source': 'pets-india'}, page_content='I love cats')],\n",
       " [Document(id='deaa49f1-19ad-48f2-9f0a-b16710fe315a', metadata={'source': 'pets-india'}, page_content='I love dogs')]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "\n",
    "retriever.batch([\"cat\",\"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8239a06",
   "metadata": {},
   "source": [
    "### Retriever As Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35e5a901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The only information provided in the context is the sentence:\\n\\n**‚ÄúI love football.‚Äù**'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message= \"\"\"\n",
    "Answer thus question using the provided context only.\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\",message)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain={\"context\":retriever, \"question\":RunnablePassthrough()}|prompt_template|model\n",
    "\n",
    "response=rag_chain.invoke(\"tell me about football\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2ba49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
